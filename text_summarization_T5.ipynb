{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cd83b312",
      "metadata": {
        "id": "cd83b312"
      },
      "outputs": [],
      "source": [
        "# we use data from https://huggingface.co/datasets/cnn_dailymail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "844f0a37",
      "metadata": {
        "id": "844f0a37"
      },
      "outputs": [],
      "source": [
        "# If we download data using hugging face dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac88af6e",
      "metadata": {
        "id": "ac88af6e",
        "outputId": "6f6ddbc9-0aec-4390-a0e8-f691d84a1d92"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# I just downloaded the smallest subset\n",
        "docs = load_dataset('cnn_dailymail', '1.0.0', split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7fdf4f1e",
      "metadata": {
        "id": "7fdf4f1e"
      },
      "outputs": [],
      "source": [
        "# Get a train set and a val set from the downloaded data\n",
        "docs_train_val = docs.train_test_split(train_size=0.05, test_size=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6967330f",
      "metadata": {
        "id": "6967330f",
        "outputId": "25019493-89f8-4900-d8c0-418969081599"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 574\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 115\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_train_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4e02735b",
      "metadata": {
        "id": "4e02735b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-30 11:29:50.872516: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-30 11:29:51.888258: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-30 11:29:51.888817: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-30 11:29:56.288438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "680c8929",
      "metadata": {
        "id": "680c8929"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Get the tokenizer - making words into numbers\n",
        "tokenizer = AutoTokenizer.from_pretrained('t5-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9f7ffab3",
      "metadata": {
        "id": "9f7ffab3"
      },
      "outputs": [],
      "source": [
        "# prefix is needed for T5\n",
        "prefix = 'summarize:'\n",
        "\n",
        "def tokenization(docs, max_article_length=1024, max_summary_length=64):\n",
        "\n",
        "    articles = [prefix + doc for doc in docs['article']]\n",
        "\n",
        "    tokenized_articles = tokenizer(articles, max_length=max_article_length, truncation=True)\n",
        "\n",
        "    tokenized_summaries = tokenizer(docs['highlights'], max_length=max_summary_length, truncation=True)\n",
        "\n",
        "    tokenized_articles['labels'] = tokenized_summaries['input_ids']\n",
        "\n",
        "    return tokenized_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67893389",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "67893389",
        "outputId": "e81308a1-1ed2-4ca1-a52b-8ad9f8a12181"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82cd5e42f22749fda24334747e6bbad8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/574 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ccafc6278d2494484dc499e95cc3565",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# tokenize the data\n",
        "tokenized_docs = docs_train_val.map(tokenization, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b18d4ffb",
      "metadata": {
        "id": "b18d4ffb"
      },
      "outputs": [],
      "source": [
        "# keep only the tokens\n",
        "tokens_docs = tokenized_docs.remove_columns(['article','highlights', 'id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9c3fde50",
      "metadata": {
        "id": "9c3fde50",
        "outputId": "3b4c2cb3-117f-404a-8d78-305d154d043a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 574\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 115\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc19b114",
      "metadata": {
        "id": "bc19b114",
        "outputId": "dd7fdab3-abd1-4c31-e719-f54e8e4271ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-30 11:30:16.048101: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "# get the model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7761c63c",
      "metadata": {
        "id": "7761c63c"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "# get a collator for data padding\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4b7f9af3",
      "metadata": {
        "id": "4b7f9af3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "# process data\n",
        "train_data = model.prepare_tf_dataset(\n",
        "    tokens_docs['train'],\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    tokenizer= tokenizer,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "val_data = model.prepare_tf_dataset(\n",
        "    tokens_docs['test'],\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    tokenizer= tokenizer,\n",
        "    collate_fn=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4c37943d",
      "metadata": {
        "id": "4c37943d",
        "outputId": "9c6805e3-954e-4ea7-e8da-789d8ae5dfd1"
      },
      "outputs": [],
      "source": [
        "# Use Adam as the optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "abedcd2c",
      "metadata": {
        "id": "abedcd2c",
        "outputId": "9f64793c-5b08-440a-be02-ac276d3cb3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "71/71 [==============================] - 995s 14s/step - loss: 2.2302 - val_loss: 1.9888\n",
            "Epoch 2/2\n",
            "71/71 [==============================] - 970s 14s/step - loss: 2.0659 - val_loss: 1.9190\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f2fbc7a5710>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_data, validation_data=val_data, epochs=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "871ef210",
      "metadata": {
        "id": "871ef210"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "067a2ae6",
      "metadata": {
        "id": "067a2ae6"
      },
      "outputs": [],
      "source": [
        "# model.save_weights('summarized_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "648427b3",
      "metadata": {
        "id": "648427b3"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "29a8a773",
      "metadata": {
        "id": "29a8a773"
      },
      "outputs": [],
      "source": [
        "# get 10 samples for evaluation\n",
        "eval_articles = docs['article'][-10:]\n",
        "eval_highlights = docs['highlights'][-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "44279c8d",
      "metadata": {
        "id": "44279c8d"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "98b8f848",
      "metadata": {
        "id": "98b8f848"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1730 > 512). Running this sequence through the model will result in indexing errors\n",
            "2023-11-30 12:03:12.417400: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a5da580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-30 12:03:12.417443: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-11-30 12:03:12.956920: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-11-30 12:03:14.810094: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        }
      ],
      "source": [
        "generated_summaries = [pipe(article) for article in eval_articles]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3f45282d",
      "metadata": {
        "id": "3f45282d",
        "outputId": "386ec65a-b24d-4be9-b458-51fdc32bb7b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(generated_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "42ab9a59",
      "metadata": {
        "id": "42ab9a59"
      },
      "outputs": [],
      "source": [
        "generated_summaries = [k[0]['summary_text'] for k in generated_summaries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9769ae2e",
      "metadata": {
        "id": "9769ae2e"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a3b3b280",
      "metadata": {
        "id": "a3b3b280",
        "outputId": "cd12fed7-0aad-4967-a809-ba085ec4b0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test case: 1\n",
            "rouge1: Score(precision=0.3018867924528302, recall=0.17777777777777778, fmeasure=0.22377622377622378)\n",
            "rouge2: Score(precision=0.057692307692307696, recall=0.033707865168539325, fmeasure=0.0425531914893617)\n",
            "rougeL: Score(precision=0.16981132075471697, recall=0.1, fmeasure=0.1258741258741259)\n",
            "--------------------------------------------------\n",
            "test case: 2\n",
            "rouge1: Score(precision=0.3333333333333333, recall=0.5882352941176471, fmeasure=0.42553191489361697)\n",
            "rouge2: Score(precision=0.22033898305084745, recall=0.3939393939393939, fmeasure=0.2826086956521739)\n",
            "rougeL: Score(precision=0.31666666666666665, recall=0.5588235294117647, fmeasure=0.4042553191489362)\n",
            "--------------------------------------------------\n",
            "test case: 3\n",
            "rouge1: Score(precision=0.265625, recall=0.4358974358974359, fmeasure=0.3300970873786408)\n",
            "rouge2: Score(precision=0.1111111111111111, recall=0.18421052631578946, fmeasure=0.13861386138613863)\n",
            "rougeL: Score(precision=0.203125, recall=0.3333333333333333, fmeasure=0.2524271844660194)\n",
            "--------------------------------------------------\n",
            "test case: 4\n",
            "rouge1: Score(precision=0.4, recall=0.3684210526315789, fmeasure=0.3835616438356164)\n",
            "rouge2: Score(precision=0.11764705882352941, recall=0.10810810810810811, fmeasure=0.11267605633802817)\n",
            "rougeL: Score(precision=0.2, recall=0.18421052631578946, fmeasure=0.1917808219178082)\n",
            "--------------------------------------------------\n",
            "test case: 5\n",
            "rouge1: Score(precision=0.7450980392156863, recall=0.8636363636363636, fmeasure=0.8)\n",
            "rouge2: Score(precision=0.74, recall=0.8604651162790697, fmeasure=0.7956989247311828)\n",
            "rougeL: Score(precision=0.7450980392156863, recall=0.8636363636363636, fmeasure=0.8)\n",
            "--------------------------------------------------\n",
            "test case: 6\n",
            "rouge1: Score(precision=0.3541666666666667, recall=0.4473684210526316, fmeasure=0.3953488372093023)\n",
            "rouge2: Score(precision=0.14893617021276595, recall=0.1891891891891892, fmeasure=0.16666666666666666)\n",
            "rougeL: Score(precision=0.2916666666666667, recall=0.3684210526315789, fmeasure=0.32558139534883723)\n",
            "--------------------------------------------------\n",
            "test case: 7\n",
            "rouge1: Score(precision=0.4117647058823529, recall=0.30701754385964913, fmeasure=0.35175879396984927)\n",
            "rouge2: Score(precision=0.10714285714285714, recall=0.07964601769911504, fmeasure=0.09137055837563451)\n",
            "rougeL: Score(precision=0.2, recall=0.14912280701754385, fmeasure=0.17085427135678394)\n",
            "--------------------------------------------------\n",
            "test case: 8\n",
            "rouge1: Score(precision=0.5428571428571428, recall=0.5, fmeasure=0.5205479452054795)\n",
            "rouge2: Score(precision=0.21739130434782608, recall=0.2, fmeasure=0.20833333333333331)\n",
            "rougeL: Score(precision=0.2, recall=0.18421052631578946, fmeasure=0.1917808219178082)\n",
            "--------------------------------------------------\n",
            "test case: 9\n",
            "rouge1: Score(precision=0.3111111111111111, recall=0.875, fmeasure=0.459016393442623)\n",
            "rouge2: Score(precision=0.2247191011235955, recall=0.6451612903225806, fmeasure=0.3333333333333333)\n",
            "rougeL: Score(precision=0.28888888888888886, recall=0.8125, fmeasure=0.4262295081967213)\n",
            "--------------------------------------------------\n",
            "test case: 10\n",
            "rouge1: Score(precision=0.5428571428571428, recall=0.2159090909090909, fmeasure=0.3089430894308943)\n",
            "rouge2: Score(precision=0.29411764705882354, recall=0.11494252873563218, fmeasure=0.16528925619834708)\n",
            "rougeL: Score(precision=0.3142857142857143, recall=0.125, fmeasure=0.17886178861788618)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "for i in range(10):\n",
        "    scores = scorer.score(eval_highlights[i], generated_summaries[i])\n",
        "    print('test case:', i+1)\n",
        "    for key in scores:\n",
        "        print(f'{key}: {scores[key]}')\n",
        "    print('-'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73870d2",
      "metadata": {
        "id": "d73870d2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
